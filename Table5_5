# Simulate "bias scores" for fairness analysis:
# We'll calculate the difference in predicted positive rates between two artificial demographic groups

# Define a binary group for fairness testing (simulate 'Group A' vs 'Group B')
df_model["Demographic_Group"] = np.where(df_model["Social_Media_Footprint"].isin(["High", "Medium"]), "Group A", "Group B")

# Feature sets
traditional_features = ["Age", "Employment_Status", "Residential_Stability", "Bank_Account_Tenure"]
alternative_features = [
    "Utility_Payment_History", "Rent_Payment_History", "Telecommunications_Payment_History",
    "Educational_Background", "Online_Shopping_Behavior", "Social_Media_Footprint",
    "Gig_Economy_Participation"
]

# Prepare model set
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(),
    "Support Vector Machine": SVC(probability=True),
    "DNN (MLPClassifier)": MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500)
}

# Function to compute bias score: difference in positive prediction rates
def compute_bias_score(X, y, group_labels, model):
    X_train, X_test, y_train, y_test, g_train, g_test = train_test_split(
        X, y, group_labels, test_size=0.2, stratify=y, random_state=42
    )

    # Setup columns
    num_cols = X.select_dtypes(include=["int64", "float64"]).columns.tolist()
    cat_cols = X.select_dtypes(include=["object"]).columns.tolist()

    numeric_transformer = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ])
    categorical_transformer = Pipeline([
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])

    preprocessor = ColumnTransformer([
        ("num", numeric_transformer, num_cols),
        ("cat", categorical_transformer, cat_cols)
    ])

    pipe = Pipeline([
        ("preprocessor", preprocessor),
        ("classifier", model)
    ])

    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)

    # Compute group-wise positive prediction rates
    group_a_rate = y_pred[g_test == "Group A"].mean()
    group_b_rate = y_pred[g_test == "Group B"].mean()
    bias_score = abs(group_a_rate - group_b_rate)
    return round(bias_score, 3)

# Compute for both feature sets
bias_results = []
for model_name, model in models.items():
    # Traditional
    X_t = df_model[traditional_features]
    y_t = df_model["Creditworthy"]
    group_t = df_model["Demographic_Group"]
    bias_trad = compute_bias_score(X_t, y_t, group_t, model)

    # Alternative
    X_a = df_model[alternative_features]
    y_a = df_model["Creditworthy"]
    group_a = df_model["Demographic_Group"]
    bias_alt = compute_bias_score(X_a, y_a, group_a, model)

    bias_results.append({
        "Model": model_name,
        "Bias Score (Traditional Data)": bias_trad,
        "Bias Score (Alternative Data)": bias_alt
    })

# Display Table 5.4
df_table_5_4 = pd.DataFrame(bias_results)
tools.display_dataframe_to_user(name="Table 5.4 - Fairness Metrics (Bias Scores)", dataframe=df_table_5_4)
